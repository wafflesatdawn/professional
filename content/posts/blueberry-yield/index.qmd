---
title: "Blueberry Yield"
date: 2023-05-08T10:05:27-07:00
format: hugo-md
jupyter: python3
draft: true
---

Intro about predicting blueberry harvest.

# Set up data
```{python}
from pathlib import Path
from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype
from fastai.tabular.all import *
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor, export_graphviz
import dtreeviz
from IPython.display import Image, display_svg, SVG

data_path = Path('./data')
df = pd.read_csv(data_path/'train.csv', low_memory=False)
df_test = pd.read_csv(data_path/'test.csv', low_memory=False)
df
```

# Data prep
```{python}
df.columns
df_test.columns
[{df[i].name:df[i].unique()} for i in df.columns]
```

Based on how few unique values they contain, the first five columns might warrant treatment as categorical variables
```{python}
df.clonesize = df.clonesize.astype('category')
df.honeybee = df.honeybee.astype('category')
df.bumbles = df.bumbles.astype('category')
df.andrena = df.andrena.astype('category')
df.osmia = df.osmia.astype('category')
```

The dependent variable we are predicting is yield, and procs are wrappers on Pandas that handle strings and missing data. This dataset does not contain strings bu the functionality are grouped together. `Categorify` is a `TabularProc` that replaces a column with a numeric categorical column. `FillMissing` is a `TabularProc` that replaces missing values with the median of the column, and creates a new Boolean column that is set to `True` for any row where the value was missing.
```{python}
dep_var = 'yield'
procs = [Categorify, FillMissing]
```

Based on the structure of the dataset, we will randomly split the data into train and validation sets
```{python}
rng = np.random.default_rng()
np.random.seed(11)
train_size = round(len(df) * .7)
train_idx = rng.integers(low=0, high=df.last_valid_index(), size=train_size)
splits = (list(train_idx), list(df.index[~train_idx]))

```

Tell TabularPandas which columns are continuous and categorical. Save the processed data for later use.
```{python}
cont,cat = cont_cat_split(df, 1, dep_var=dep_var)
to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits)
to.show(3)
save_pickle(data_path/'to.pkl',to)
```

# Creating decision trees
First define x and y, the independent and dependent variables. Then create the decision tree.
```{python}
to = load_pickle(data_path/'to.pkl')
trn_xs,trn_y = to.train.xs,to.train.y
valid_xs,valid_y = to.valid.xs,to.valid.y
m = DecisionTreeRegressor(max_leaf_nodes=4)
m.fit(trn_xs, trn_y);
```

## Visualization
This function visualizes the decision tree for the training x data that's been passed to the tabularpandas. The first node is before anything has been done. The value is the mean of the variable we're trying to predict, yield, and the sample is the length of the dataframe.
```{python}
import graphviz

def draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs):
    s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True,
                      special_characters=True, rotate=False, precision=precision, **kwargs)
    return graphviz.Source(re.sub('Tree {', f'Tree {{ size={size}; ratio={ratio}', s))
draw_tree(m, trn_xs, size=10)
```
We can confirm by checking summary statistics on the training set
```{python}
df.iloc[train_idx].describe()
```
The next two nodes come from bisecting the dataset by values for fruitset above and below 0.5.

Alternative visualization using dtreeviz, showing the distribution of data along with the bisecting lines
```{python}
dtreeviz.model(m, X_train=trn_xs, y_train=trn_y, feature_names=df.columns, target_name=dep_var).view()
```

These previews were limited to just 4 nodes but now we will remove it
```{python}
m = DecisionTreeRegressor()
m.fit(trn_xs, trn_y);
```

# Model performance evaluation
The Kaggle competition that this data was taken from will evaluate models based on mean absolute error

{{< katex >}}
`\(( MAE = \frac{1}{n}\sum_{i=1}^n|x_1 - y_1| \))`{=markdown}

where each `\\( x_i \\)`{=markdown} represents the predicted target, `\\( y_i \\)`{=markdown}  represents the ground truth, and `\\( n \\)`{=markdown} is the number of rows in the test set.

```{python}
def mae(predictions, actuals):
    """calculate the mean absolute error between prediction and actual values

    Args:
        predictions (Series): from training set
        actuals (Series): from validation set

    Returns:
        _type_: float
    """
    return abs(predictions - actuals).sum() / len(predictions)
```
Generating our predictions from the model and taking the MAE:
```{python}
predictions = m.predict(trn_xs)
mae(predictions, trn_y)
```
Checking it against our validation set:
```{python}
mae(m.predict(valid_xs), valid_y)
```

A mean absolute error of 0 indicates overfitting, as the default setting for sklearn is to continue splitting nodes until they run out. The total nodes, or leaves, in the tree is almost as high as the total rows in the training set:
```{python}
m.get_n_leaves(), len(trn_xs)
```

Changing it to 25 modes will fix the problem, bringing the MAE closer to the validation set.
```{python}
m = DecisionTreeRegressor(min_samples_leaf=25)
m.fit(trn_xs, trn_y)
mae(m.predict(trn_xs), trn_y)
mae(m.predict(trn_xs), trn_y)
```

# Tree becomes forest
Decision trees offer a balance of generalization and accuracy, but they are on opposite ends of a fulcrum. Limiting the size of the tree means it generalizes well at the expense of accuracy and vice versa. To overcome this compromise, data scientists started using a new technique called random forests, extending the analogy. The intuition behind random forests echoes the central limit theorem: an aggregated measure derived from several samples is more accurate than any of the individual samples. However, random forests has specific criteria
- subset and bootstrap data from the training set
- randomly use different subsets of columns when choosing splits in each decision tree

Creating a random forest will be similar to the decision trees from earlier
```{python}
def rf(xs, y, n_estimators=40, max_samples=2000, max_features=0.5, min_samples_leaf=5, **kwargs):
    """generate a random forest

    Args:
        xs (DataFrame): independent variables
        y (Series): dependent variable
        n_estimators (int, optional): number of trees. Defaults to 40.
        max_samples (_type_, optional): rows to sample for training each tree. Defaults to 2000.
        max_features (float, optional): number of features to consider when looking for the best split. Defaults to 0.5, meaning half.
        min_samples_leaf (int, optional): minimum number of samples in each leaf. Defaults to 5.

    Returns:
        _type_: _description_
    """
    return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators,
        max_samples=max_samples, max_features=max_features,
        min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y)
```

Comparing the training results to the validation set, MAE using random forest gains a slight improvement over the single large decision tree
```{python}
m = rf(trn_xs, trn_y)
mae(m.predict(trn_xs), trn_y), mae(m.predict(valid_xs), valid_y)
```

The random forest we created created has 40 trees, each of which was can be accessed by indexing from `m.estimators_`, which returns a list of all the predicted yields
```{python}
m.estimators_[3].predict(valid_xs)
```
Numpy's `stack` method allows for easy manipulations of arrays, quickly moving values from one to another as seen its documentation examples:
```python
>>> arrays = [np.random.randn(3, 4) for _ in range(10)]
>>> np.stack(arrays, axis=0).shape
(10, 3, 4)
>>> np.stack(arrays, axis=1).shape
(3, 10, 4)
>>> np.stack(arrays, axis=2).shape
(3, 4, 10)
>>> a = np.array([1, 2, 3])
>>> b = np.array([4, 5, 6])
>>> np.stack((a, b))
array([[1, 2, 3],
       [4, 5, 6]])
>>> np.stack((a, b), axis=-1)
array([[1, 4],
       [2, 5],
       [3, 6]])
```
The resulting output from `stack` is another numpy array, giving us access to the mean method. Usually taking the mean of an array reduces the dimensions and returns just a scalar. However, passing an optional argument will instead take the mean along either axis.
```{python}
preds = np.stack([t.predict(valid_xs) for t in m.estimators_])
print(preds.mean())
print(preds.mean(0))
print(preds.mean(1))
assert len(trn_xs) == len(preds.mean(0))
```
Using this gives back the predicted yields for each decision tree (axis 1) or the entire dataset (axis 0), which will return the same MAE from as earlier
```{python}
mae(preds.mean(0), valid_y)
```
This can then be used to plot how the MAE changes as more decision trees get added, starting from 0 all the way to 40, the maximun number we specified.
```{python}
plt.plot([mae(preds[:i+1].mean(0), valid_y) for i in range(40)])
```