('## Intro\nTensors are the foundational building blocks of machine learning and their nuances are worth spending some time on. This post is meant to be a refresher on pytorch tensors, pulling together information from various sources.\n\n## Creating tensors\n```{python}\nimport torch\nimport math\n\nx = torch.empty(3,4)\nprint(type(x))\nprint(x)\n```\nTo start off, the [pytorch docs](https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html) include a great primer that begins by initializing a tensor just like the one above:\n> Let’s unpack what we just did:\n> - We created a tensor using one of the numerous factory methods attached to the torch module.\n> - The tensor itself is 2-dimensional, having 3 rows and 4 columns.\n> - The type of the object returned is torch.Tensor, which is an alias for torch.FloatTensor; by default, PyTorch tensors are populated with 32-bit floating point numbers. (More on data types below.)\n> - You will probably see some random-looking values when printing your tensor. The torch.empty() call allocates memory for the tensor, but does not initialize it with any values - so what you’re seeing is whatever was in memory at the time of allocation.\n\nHowever, uncontrolled random tensor starting values are often less useful than all zeros or ones. If randomness is desired, it can be fixed to be the same every time using `manual_seed`:\n```{python}\nzeros = torch.zeros(2, 3)\nprint(zeros)\n\nones = torch.ones(2, 3)\nprint(ones)\n\ntorch.manual_seed(1729)\nrandom = torch.rand(2, 3)\nprint(random)\n```\n\n## Comparison with numpy arrays\nTensors are higher dimensional analogues to numpy arrays, sharing many qualities such as their objectives, apis, syntax, and functionality. Notable differences do exist however:\n- tensors have specialized gpu processing support to accelerate operations\n- tensors cannot be \'jagged\', meaning having dimensions of different length\n```python\nx + zeros\n\nRuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1\n```\n- all data must be the same type\n```{python}\nimport numpy as np\nnp.array([1, \'two\', 3])\n```\n```python\ntorch.rand([1, \'two\', 3])\nTypeError: rand(): argument \'size\' must be tuple of ints, but found element of type str at pos 2\n```\n### Going beyond 3d\nHigher dimensionality is difficult to visualize and often invites people to start drawing volumes and hypercubes. It\'s simply a consequence of \'dimension\' in common usage as well as a sign of how far the cartesian system has permeated. How do you think beyond 3D? We can go \'back to 2D\', in a sense. Forget about trying to imagine the data in shapes, connecting lines between them, and instead consider tensor dimensionaliy simply as arrays of arrays (or lists of lists).\n\nObserve how the output grows when we create tensors of increasing size:\n```{python}\ntorch.ones(3,3)\n```\n```{python}\ntorch.ones(3,3,3)\n```\n```{python}\ntorch.ones(3,3,3,3)\n```\nThe last output is actually truncated but if you do this on a terminal you\'ll get the entirety. There are a couple of ways to read the pattern that emerges from playing around with this. These examples kept all the arguments identical but modifying the last one makes it clearer:\n```{python}\ntorch.ones(2,3,1,5)\n```\nThe last number determines the size of the innermost elements of the tensor (here, 5) while the first number determines the number of groups at the highest level. So, if set to 1, \n\n\n Higher dimension tensors have additional\nTensors of rank 2 and 3 are old hat--it\'s easy to see how the 9 numbers have become 27: . Rank 4 is where it starts to get hairy. \n\n```{python}\ntorch.ones(5)\n```\n```{python}\ntorch.ones(5,4)\n```\n```{python}\ntorch.ones(5,4,3)\n```\n```{python}\ntorch.ones(5,4,3,2)\n```\nThere are a couple of patterns we can see at this point, and they come from different directions. First, obviously, the output in the terminal is growing longer. But at the same time, parts of it seem to be shrinking.\n\n## Operations with tensors\nAs seen above, it\'s not possible to perform operations on tensors when they have different shapes. "Shape" means having the same number of dimensions and the same number of items in each dimension. For example, these two are not the same shape:\n```{python}\n# tensor(1,2) - tensor(2,1)\n```\n### Element access\n\n\n\n\n\n', ResourcesDict(None, {'metadata': ResourcesDict(None, {'name': 'Notebook'}), 'output_extension': '.md', 'deprecated': <function deprecated at 0x000002A1FE222E80>, 'outputs': {}, 'raw_mimetypes': ['text/markdown', 'text/html', ''], 'global_content_filter': {'include_code': True, 'include_markdown': True, 'include_raw': True, 'include_unknown': True, 'include_input': True, 'include_output': True, 'include_output_stdin': False, 'include_input_prompt': True, 'include_output_prompt': True, 'no_prompt': False}}))