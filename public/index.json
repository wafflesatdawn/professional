[{"content":"Suppose that you started a new job and the first thing that awaited you was a vague request from the CEO: \u0026ldquo;I really want to know what people think about how to prepare sand dabs\u0026rdquo;. No further context is given because she is suddenly met by other people and has turned her focus to them, walking off. Your new manager is still around to say that the question is actually important, so you should try to produce an answer. How would you start?\nEstablishing context # Displays of whim and out-of-the-box thinking are not unheard of, and can be considered as tests or opportunities. The first thing to do is start asking a couple questions to establish context:\nWhat is the scope of the request? By \u0026ldquo;people\u0026rdquo; was the CEO referring to the employees on site? Or at every site, if the company is a multinational? Or even broader, the entire country? Having selected a population from one of those options, how would their opinions be evaluated? Does everyone\u0026rsquo;s input have equal weight? What kind of sampling strategy should be used? In addition, there are also operational details:\nHow does the CEO want to receive status updates about the query\u0026ndash;impromptu chat (in-person or Teams/Slack), email, or meeting? What are the time constraints for delivering results? And how should results be presented? When in doubt, async is your out # Since it\u0026rsquo;s not possible to get immediate answers to these questions, the best alternative would be to adopt an asynchronous mindset, with the reasoning that you don\u0026rsquo;t want to sit around doing nothing until you get a response. So, take each of the questions above and write a memo that goes through them, listing potential options and which of them you\u0026rsquo;ll be working with.\nHow should you winnow these options? There are factors to consider, such as practicality, that will tilt the balance in favor of one over another. Scope is an easy example. Preparing to survey a large, globally distributed population would take no small effort, so starting with on-site only is reasonable, allowing for a pivot to the rest if needed. And speaking of surveys, they are ideal tools for collecting data\u0026ndash;especially in this case because of the subjectivity (what people think) and narrowness of topic (preparing sand dabs). There\u0026rsquo;s no reason to assume different weighting of inputs for this, but it\u0026rsquo;s a good idea to always consider how the quality of your data varies across different sources. Choice of sampling strategy will depend heavily on the number of people on site. Ten employees is trivial, but a hundred or a thousand employees is another matter.\nThe importance of earnest sampling # Sampling properly is so important that it deserves special mention. Because it happens at the start of an experiment or project, before you start doing any fancy work, it\u0026rsquo;s easy to gloss it over. I might be biased but I like to give greater thought to the process that forms the entire basis of my datasets.\nIf the site had 100 employees, it might still be tempting to survey everyone and just call it a day because the total number seems small and you\u0026rsquo;ll get all the data you need. Or will you? A survey about making sand dab seems so trivial, being sent from a new employee no less, will it simply get ignored by the majority except for maybe a handful of people who are passionate about seafood? In this example, the CEO might not care that she\u0026rsquo;s only hearing from just a few people, but in real studies, this kind of response would be nonrepresentative and any conclusions drawn from it would be dubious.\nNow suppose the company was in the tech industry and had a large gender imbalance of 8:2 male-to-females. What are the assumptions and beliefs we hold about the culinary virtuousity of the typical man or woman? If we say that in the year 2023, they must be roughly equal by now, then we will be okay with sampling them as a homogenous group. However, if we believe their respective abilities and knowledge of cooking are skewed\u0026ndash;or even if we\u0026rsquo;re merely curious whether gender plays a role\u0026ndash;then we should use stratified sampling so our responses are labeled as coming from each group.\nIf using stratified sampling, then should each group have the same number of samples (disproportionate) or should it vary according to the group size (proportionate)? A proportionate sample here would be 8 men and 2 women (not mentioning non-binary for the sake of brevity). If you believe women know more about cooking than men, then the results you\u0026rsquo;ll be presenting to the CEO here would be uninspired or worse\u0026ndash;maybe lots of responses such as \u0026ldquo;microwave it and add hot sauce\u0026rdquo;. Proportionate samples reflect the population at the expense of groups. Conversely, a disproportionate sample here would be 5 of each gender, and the responses will be markedly different, maybe suggesting white wine or sous vide. Disproportionate samples amplify underrepresented groups.\nStatus # \u0026ldquo;Status update\u0026rdquo; is a phrase that can be colored several ways, depending on the circumstance. It could be just procedural and of little consequence: how UPS is working on your next delivery of Tide Pods. It could be bureaucratic and of much greater significance: the various stages of closing a real estate transaction. But it can also accrue a weight made heavy by negative emotions. Avoid this undesireable state by striving to make status updates more procedural or bureaucratic. Whether meeting, email, or chat, they should be proactive and regular.\nTime # In the absence of clear directives, we must make do with what we have, and here it boils down to common sense and educated guessing. Many units of measure exist for time. If you try to make a turnaround on the sand dabs within minutes or hours, will you have had enough time for it be of much substance? What about a day or several days? Weeks and months? The last two are obviously unlikely for this example, but weighing all the timeperiods as possible options produces a strong contrast that makes it obvious which are the most reasonable\u0026ndash;a day or several days. Depending on the work in question, there may be contending options for the most reasonable time estimate or constraint, but that\u0026rsquo;s not as much of an issue as long as they use different units of measures: days against weeks, weeks against months. Longer time horizons invite debate over strategic prioritization, which is more productive than quibbling over small degrees of precision.\nThe end # All said and done, I do not actually know any recipes for sand dabs. This contrivance came from a dream which I was able to remember as I woke up today, and since the memory was so fresh, it spurred me to jot down some thoughts, which morphed into a blog post. As for the dream itself, I can offer no explanation better than dream logic. I don\u0026rsquo;t even think I\u0026rsquo;ve had sand dabs more than once or twice, and it was from a little restaurant far off in Monterey. They had lightly floured and sauted it in butter, if I recall correctly.\n","date":"Jul 5, 2023","permalink":"/professional/posts/asking-about-sand-dabs/","section":"Posts","summary":"Suppose that you started a new job and the first thing that awaited you was a vague request from the CEO: \u0026ldquo;I really want to know what people think about how to prepare sand dabs\u0026rdquo;.","title":"Asking About Sand Dabs"},{"content":"","date":"Jul 5, 2023","permalink":"/professional/","section":"luminal transubstantiation","summary":"","title":"luminal transubstantiation"},{"content":"When you get your own OS-specific errors you have to start wondering whether the mere act of programming on Windows is a mistake.\nAn attempt has been made to start a new process before the\rcurrent process has finished its bootstrapping phase.\rThis probably means that you are not using fork to start your\rchild processes and you have forgotten to use the proper idiom\rin the main module:\rif __name__ == \u0026#39;__main__\u0026#39;:\rfreeze_support()\r...\rThe \u0026#34;freeze_support()\u0026#34; line can be omitted if the program\ris not going to be frozen to produce an executable. This error snippet comes from Pytorch\u0026rsquo;s implementation of multiprocessing and can show up during execution even when your code doesn\u0026rsquo;t explicitly do any\u0026ndash;simply specifying a paramter such as num_workers is an implicit use of the feature and invites its caveats. The warning refers to the use of fork and starting subprocesses, but the choice of using it is not explicit either and simply is a consequence of what OS your machine is running.\nAccording to the PyTorch docs:\nSince workers rely on Python multiprocessing, worker launch behavior is different on Windows compared to Unix.\nOn Unix, fork() is the default multiprocessing start method. Using fork(), child workers typically can access the dataset and Python argument functions directly through the cloned address space. On Windows or MacOS, spawn() is the default multiprocessing start method. Using spawn(), another interpreter is launched which runs your main script, followed by the internal worker function that receives the dataset, collate_fn and other arguments through pickle serialization. Unix users won\u0026rsquo;t see this specific error because its implementation allows direct access to data and variables. Elsewhere, the spawn implementation causes independent processes to start up and when that happens, all the code will be run a second (or nth) time unless precautions are taken:\nThis separate serialization means that you should take two steps to ensure you are compatible with Windows while using multi-process data loading:\nWrap most of you main script’s code within if name == \u0026lsquo;main\u0026rsquo;: block, to make sure it doesn’t run again (most likely generating error) when each worker process is launched. You can place your dataset and DataLoader instance creation logic here, as it doesn’t need to be re-executed in workers. Make sure that any custom collate_fn, worker_init_fn or dataset code is declared as top level definitions, outside of the main check. This ensures that they are available in worker processes. (this is needed since functions are pickled as references only, not bytecode.) If the if __name__ == '__main__' guard idiom referenced seems novel, then your scripts have mostly been self-contained until now. Many if not all Python libraries make use of this to provide executable script functionality while still being importable as modules. The name variable is a Python default that\u0026rsquo;s automatically set to \u0026lsquo;main\u0026rsquo; for the script that is directly executed. The idiom simply takes advantage of that fact to create a conditional, under which you place the separated code that\u0026rsquo;s intended to execute only when the script is run directly.\nIn the PyTorch example, code inside the guard won\u0026rsquo;t be run again as the multiprocessing workers launch, but it\u0026rsquo;s also correct to say that the same code won\u0026rsquo;t run upon being imported to another script. Anything outside the guard, such as a df or other global variable, will execute during import to other scripts.\n","date":"May 25, 2023","permalink":"/professional/posts/pytorch-multiprocessing/","section":"Posts","summary":"When you get your own OS-specific errors you have to start wondering whether the mere act of programming on Windows is a mistake.","title":"Multiprocessing errors in PyTorch"},{"content":"\rA recent Kaggle competition was launched that asks users to predict harvest yields from wild blueberries growing in the hills and mountains of the Northeast. As you might imagine, both the harvest and the subsequent harvest of data are not easy to come by. The dataset is actually generated from synthetic data which is itself based on a simulation of harvests that\u0026rsquo;s been refined for 30 years. Background details aside, the dataset will be a good example to use for tabular modeling, where the objective is to predict one column\u0026rsquo;s values based on values from the others. Tabular modeling includes decision trees and their evolved form, random forests, both of which will be focused on here using scikit-learn.\nAlthough tabular modeling is not as attention-grabbing as NLP and image recognition, it offers many strengths in its own right for structured data:\nfaster training ease of interpretation unbound by hardware constraints at scale (eg: tensor cores) less hyperparameter tuning That said, there are two exceptions to this: high cardinality in important categorical variables presence of data that would be handled much better by neural networks, such as columns containing plain text Unstructured data will always be the domain of deep learning, but for structured data the contest is less clear and it\u0026rsquo;s worthwhile to consider tabular modeling\u0026ndash;at the very least as a baseline.\nSetting up data # from pathlib import Path from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype from fastai.tabular.all import * from sklearn.ensemble import RandomForestRegressor from sklearn.tree import DecisionTreeRegressor, export_graphviz import dtreeviz from IPython.display import Image, display_svg, SVG import warnings import seaborn as sns # suppress the warning message from sklearn warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;, category=UserWarning) data_path = Path(\u0026#39;./data\u0026#39;) df = pd.read_csv(data_path/\u0026#39;train.csv\u0026#39;, low_memory=False) As described by Kaggle, this dataset is very small\u0026ndash;barely over a megabyte\u0026ndash;and is intended to be a lighter complement to its intenser options. Here we see just a handful of columns, many of which are related, and less than 50000 rows across both the training and test sets.\nData prep # [{df[i].name:df[i].unique()[:10]} for i in df.columns] [{'id': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64)},\r{'clonesize': array([25. , 12.5, 37.5, 20. , 10. , 40. ])},\r{'honeybee': array([ 0.5 , 0.25 , 0.75 , 0.537, 0. , 18.43 , 6.64 ])},\r{'bumbles': array([0.25 , 0.38 , 0.117, 0.058, 0.56 , 0.065, 0. , 0.585, 0.042,\r0.293])},\r{'andrena': array([0.75 , 0.5 , 0.63 , 0.38 , 0.25 , 0.409, 0.707, 0. , 0.24 ,\r0.56 ])},\r{'osmia': array([0.5 , 0.63 , 0.75 , 0.25 , 0.38 , 0.058, 0.117, 0.62 , 0.585,\r0. ])},\r{'MaxOfUpperTRange': array([69.7, 86. , 77.4, 94.6, 89. , 79. ])},\r{'MinOfUpperTRange': array([42.1, 52. , 46.8, 57.2, 39. ])},\r{'AverageOfUpperTRange': array([58.2, 71.9, 64.7, 79. , 65.6])},\r{'MaxOfLowerTRange': array([50.2, 62. , 55.8, 68.2, 66. , 52. ])},\r{'MinOfLowerTRange': array([24.3, 30. , 27. , 33. , 28. , 25. , 31. ])},\r{'AverageOfLowerTRange': array([41.2, 50.8, 45.8, 55.9, 45.3])},\r{'RainingDays': array([24. , 34. , 1. , 16. , 3.77, 26. ])},\r{'AverageRainingDays': array([0.39, 0.56, 0.1 , 0.26, 0.06, 0.25, 0.07, 0.14])},\r{'fruitset': array([0.4250109 , 0.44490828, 0.55292683, 0.56597648, 0.57967664,\r0.56523939, 0.49873 , 0.61988773, 0.53255682, 0.34006335])},\r{'fruitmass': array([0.41754541, 0.42205139, 0.47085288, 0.47813655, 0.49416475,\r0.4843495 , 0.44219309, 0.52950157, 0.46536689, 0.38176787])},\r{'seeds': array([32.46088718, 33.85831713, 38.34178123, 39.46756134, 40.48451183,\r40.55501923, 35.51753863, 42.19101338, 36.16604354, 28.76356526])},\r{'yield': array([4476.81146, 5548.12201, 6869.7776 , 6880.7759 , 7479.93417,\r7267.28344, 5739.68029, 7920.06175, 6465.37205, 3519.43131])}]\rBased on how few unique values they contain, the first five columns might warrant treatment as categorical variables, so we can mark them as such\ndf.clonesize = df.clonesize.astype(\u0026#39;category\u0026#39;) df.honeybee = df.honeybee.astype(\u0026#39;category\u0026#39;) df.bumbles = df.bumbles.astype(\u0026#39;category\u0026#39;) df.andrena = df.andrena.astype(\u0026#39;category\u0026#39;) df.osmia = df.osmia.astype(\u0026#39;category\u0026#39;) The dependent variable we are predicting is yield, and procs are wrappers on Pandas that handle strings and missing data. This dataset does not contain strings bu the functionality are grouped together. Categorify is a TabularProc that replaces a column with a numeric categorical column. FillMissing is a TabularProc that replaces missing values with the median of the column, and creates a new Boolean column that is set to True for any row where the value was missing.\ndep_var = \u0026#39;yield\u0026#39; procs = [Categorify, FillMissing] Based on the structure of the dataset, we will randomly split the data into train and validation sets\nrng = np.random.default_rng() np.random.seed(11) train_size = round(len(df) * .7) train_idx = rng.integers(low=0, high=df.last_valid_index(), size=train_size) splits = (list(train_idx), list(df.index[~train_idx])) Tell TabularPandas which columns are continuous and categorical. Save the processed data for later use.\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var) to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) save_pickle(data_path/\u0026#39;to.pkl\u0026#39;,to) Creating decision trees # First define x and y, the independent and dependent variables. Then create the decision tree.\nto = load_pickle(data_path/\u0026#39;to.pkl\u0026#39;) trn_xs,trn_y = to.train.xs,to.train.y valid_xs,valid_y = to.valid.xs,to.valid.y m = DecisionTreeRegressor(max_leaf_nodes=4) m.fit(trn_xs, trn_y); Visualization # This function visualizes the decision tree for the training x data that\u0026rsquo;s been passed to the tabularpandas. The first node is before anything has been done. The value is the mean of the variable we\u0026rsquo;re trying to predict, yield, and the sample is the length of the dataframe.\nimport graphviz def draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs): s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True, special_characters=True, rotate=False, precision=precision, **kwargs) return graphviz.Source(re.sub(\u0026#39;Tree {\u0026#39;, f\u0026#39;Tree {{ size={size}; ratio={ratio}\u0026#39;, s)) draw_tree(m, trn_xs, size=10) The next two nodes come from bisecting the dataset by values for fruitset above and below 0.5.\nAlternative visualization using dtreeviz, showing the distribution of data along with the bisecting lines\ndtreeviz.model(m, X_train=trn_xs, y_train=trn_y, feature_names=df.columns, target_name=dep_var).view() These previews were limited to just 4 nodes but now we will remove it\nm = DecisionTreeRegressor() m.fit(trn_xs, trn_y); Performance evaluation # The Kaggle competition that this data was taken from will evaluate submissions based on mean absolute error\n$$ MAE = \\frac{1}{n}\\sum\\limits_{i=1}^n{|x_i - y_i|} $$\nwhere each \\( x_i \\) represents the predicted target, \\( y_i \\) represents the ground truth, and \\( n \\) is the number of rows in the test set.\ndef mae(predictions, actuals): \u0026#34;\u0026#34;\u0026#34;calculate the mean absolute error between prediction and actual values Args: predictions (Series): from training set actuals (Series): from validation set Returns: _type_: float \u0026#34;\u0026#34;\u0026#34; return abs(predictions - actuals).sum() / len(predictions) Generating our predictions from the model and taking the MAE:\npredictions = m.predict(trn_xs) mae(predictions, trn_y) 0.0\rChecking it against our validation set:\nmae(m.predict(valid_xs), valid_y) 280.6955310562753\rA mean absolute error of 0 indicates overfitting, as the default setting for sklearn is to continue splitting nodes until they run out. The total nodes, or leaves, in the tree is almost as high as the total rows in the training set:\nm.get_n_leaves(), len(trn_xs) (7345, 10702)\rChanging it to 25 modes will fix the problem, bringing the MAE closer to the validation set.\nm = DecisionTreeRegressor(min_samples_leaf=25) m.fit(trn_xs, trn_y) mae(m.predict(trn_xs), trn_y) 334.6532272376639\rTrees become forest # Decision trees offer a balance of generalization and accuracy, but they are on opposite ends of a fulcrum. Limiting the size of the tree means it generalizes well at the expense of accuracy and vice versa. To overcome this compromise, data scientists started using a new technique called random forests, extending the analogy. The intuition behind random forests echoes the central limit theorem: an aggregated measure derived from several samples is more accurate than any of the individual samples. However, random forests has specific criteria\nsubset and bootstrap data from the training set randomly use different subsets of columns when choosing splits in each decision tree Creating a random forest # The setup will be similar to the decision trees from earlier, specifying some of the same limits\ndef rf(xs, y, n_estimators=40, max_samples=2000, max_features=0.5, min_samples_leaf=5, **kwargs): \u0026#34;\u0026#34;\u0026#34;generate a random forest Args: xs (DataFrame): independent variables y (Series): dependent variable n_estimators (int, optional): number of trees. Defaults to 40. max_samples (_type_, optional): rows to sample for training each tree. Defaults to 2000. max_features (float, optional): number of features to consider when looking for the best split. Defaults to 0.5, meaning half. min_samples_leaf (int, optional): minimum number of samples in each leaf. Defaults to 5. Returns: _type_: _description_ \u0026#34;\u0026#34;\u0026#34; return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) Comparing the training results to the validation set, MAE using random forest gains a slight improvement over the single large decision tree\nm = rf(trn_xs, trn_y) mae(m.predict(trn_xs), trn_y), mae(m.predict(valid_xs), valid_y) (320.97567434528906, 340.5833703323208)\rFurther inspection of random forest performance # The random forest we created created has 40 trees, each of which was can be accessed by indexing from m.estimators_, which returns a list of all the predicted yields\nm.estimators_[3].predict(valid_xs) array([2630.4117296 , 4527.62113444, 5998.69013672, ..., 7808.58474392,\r3219.02983398, 7891.09232955])\rNumpy\u0026rsquo;s stack method allows for easy manipulations of arrays, quickly moving values from one to another as seen its documentation examples:\n\u0026gt;\u0026gt;\u0026gt; arrays = [np.random.randn(3, 4) for _ in range(10)] \u0026gt;\u0026gt;\u0026gt; np.stack(arrays, axis=0).shape (10, 3, 4) \u0026gt;\u0026gt;\u0026gt; np.stack(arrays, axis=1).shape (3, 10, 4) \u0026gt;\u0026gt;\u0026gt; np.stack(arrays, axis=2).shape (3, 4, 10) \u0026gt;\u0026gt;\u0026gt; a = np.array([1, 2, 3]) \u0026gt;\u0026gt;\u0026gt; b = np.array([4, 5, 6]) \u0026gt;\u0026gt;\u0026gt; np.stack((a, b)) array([[1, 2, 3], [4, 5, 6]]) \u0026gt;\u0026gt;\u0026gt; np.stack((a, b), axis=-1) array([[1, 4], [2, 5], [3, 6]]) The resulting output from stack is another numpy array, giving us access to the mean method. Usually taking the mean of an array reduces the dimensions and returns just a scalar. However, passing an optional argument will instead take the mean along either axis.\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_]) len(trn_xs) == len(preds.mean(0)) True\rUsing this gives back the predicted yields for each decision tree (axis 1) or the entire dataset (axis 0), which will return the same MAE from as earlier\nmae(preds.mean(0), valid_y) 340.5833703323208\rThis can then be used to plot how the MAE exponentially improves as more decision trees get added, starting from 0 all the way to 40, the maximun number that we specified.\nplt.plot([mae(preds[:i+1].mean(0), valid_y) for i in range(40)]) The model also offers oob_prediction_, which is like a miniature validation set. Because each tree in a random forest trains on a different subset of data, all data outside each subset is available to use in checking how well it generalizes.\nmae(m.oob_prediction_, trn_y) 347.18101643052483\rModel interpretation # Using the same stack from above lets us look at the random forest in greater detail. As a reminder, it has as many rows as there are trees in the model, and each row contains the subset of data available to each tree\npreds.shape (40, 10702)\rTo start off, since each decision tree is independent of the others, how do their predictions vary?\nStandard deviation # We can calculate the standard deviation for each tree along axis 0, as with the mean earlier\npreds_std = preds.std(0) sns.displot(preds_std) The distribution is reasonably narrow for most of the data but a long right tail goes as high as 3-4 times the median.\nImmportance of features # Next, we can take advantage of the feature_importances_ attribute from sklearn. (Side note: there is considerable looseness of usage regarding machine learning jargon such that the library includes its own glossary amongst other glossaries created by others.) It provides the importance that the model assigns to each feature of the dataset.\ndef rf_feat_importance(m, df): return pd.DataFrame({\u0026#39;cols\u0026#39;:df.columns, \u0026#39;imp\u0026#39;:m.feature_importances_} ).sort_values(\u0026#39;imp\u0026#39;, ascending=False) fi = rf_feat_importance(m, trn_xs) sns.barplot(fi, x=\u0026#39;imp\u0026#39;, y=\u0026#39;cols\u0026#39;, orient=\u0026#39;h\u0026#39;) \u0026lt;AxesSubplot: xlabel='imp', ylabel='cols'\u0026gt;\rThe model seems to think that only 3 things are important for prediction yield, and by a large margin\nLow importance variables # Considering the huge disparity in importance, we should try retraining the model with just a subset, perhaps the top three.\ntop3 = fi.nlargest(3, \u0026#39;imp\u0026#39;).cols trn_xs_imp = trn_xs[top3] valid_xs_imp = valid_xs[top3] m = rf(trn_xs_imp, trn_y) mae(m.predict(trn_xs_imp), trn_y) 336.3376379828103\rMAE is slightly worse, but in a dataset with significantly more columns, we would have good reason to narrow our analysis of factors using the quantified importance as cutoff criteria. Here we will decide to keep the extra columns in exchange for the better score.\nFinal predictions # To submit to Kaggle, all we need to do is load the test data and feed it to our random forest model, making sure the order and number of columns is consistent between both datasets. To do that, we need to repeat the relevant data processing steps and redefine the model.\nm = rf(trn_xs, trn_y) df_test = pd.read_csv(data_path/\u0026#39;test.csv\u0026#39;, low_memory=False) test_to = TabularPandas(df_test, procs, cat, cont) final_preds = m.predict(test_to[trn_xs.columns]) submission = pd.DataFrame() submission[\u0026#39;yield\u0026#39;] = final_preds submission.index += 15289 submission.to_csv(\u0026#34;submission.csv\u0026#34;, index=True, header=True, index_label=\u0026#34;id\u0026#34;) Limitations of tabular modeling # Due to the fundamental design of random forests, they are bad at extrapolating. Because a random forest averages the predictions of its trees, each of which average the the values in their leaves, predictions will never go beyond the range of values in the training data. The good news is that the problem can be mitigated by checking whether our model\u0026rsquo;s predictive power hinges on conditions that aren\u0026rsquo;t shared between the training and validation sets.\nTo achieve this we\u0026rsquo;ll simply create a new dependent variable that marks whether data is from the training set or the validation set and use a random forest to predict it.\ncombo = pd.concat([trn_xs, valid_xs]) is_valid = np.array([0]*len(trn_xs) + [1]*len(valid_xs)) m = rf(combo, is_valid) rf_feat_importance(m, combo)[:5] cols imp 5 id 0.210471 14 fruitset 0.179450 16 seeds 0.171691 15 fruitmass 0.169624 4 osmia 0.047923 We then look at the feature importance of the new model. For each feature, the higher the importance, the greater the disparity between validation and training datasets. In this case, there\u0026rsquo;s not much of a discrepancy betweeen validation and training data, with the highest importance at 0.2. This result reflects the random splitting of the original data, but if it were structured differently, such as a time series, then random splitting would not have been the ideal choice, leading to greater chance of bigger discrepancies.\nDeep learning version # Now let\u0026rsquo;s see what kind of MAE a neural network would produce. We need to take the same steps for data prep as with the tabular models\ndf_nn = pd.read_csv(data_path/\u0026#39;train.csv\u0026#39;, low_memory=False) df_nn.clonesize = df_nn.clonesize.astype(\u0026#39;category\u0026#39;) df_nn.honeybee = df_nn.honeybee.astype(\u0026#39;category\u0026#39;) df_nn.bumbles = df_nn.bumbles.astype(\u0026#39;category\u0026#39;) df_nn.andrena = df_nn.andrena.astype(\u0026#39;category\u0026#39;) df_nn.osmia = df_nn.osmia.astype(\u0026#39;category\u0026#39;) cont_nn,cat_nn = cont_cat_split(df_nn, dep_var=dep_var) Decision trees and random forests don\u0026rsquo;t care about normalized data but neural networks definitely do. Additionally, we set the batch size pretty high to 1024 because memory consumption won\u0026rsquo;t be very high with tabular data.\nprocs_nn = [Categorify, FillMissing, Normalize] to_nn = TabularPandas(df_nn, procs_nn, cat_nn, cont_nn, splits=splits, y_names=dep_var) dls = to_nn.dataloaders(102) y = to_nn.train.y y.min(),y.max() (1945.5306, 8969.401)\rWe also set the y range close to the minimum and maximum observed values for y to initialize the tabular_learner with and find the best learning rate\ny = to_nn.train.y y.min(),y.max() learn = tabular_learner(dls, y_range=(1900, 9000), n_out=1, loss_func=F.mse_loss) learn.lr_find() SuggestedLRs(valley=0.0014454397605732083)\rUsing a learning rate of ___, we get these training results\nlearn.fit_one_cycle(5, 1e-2) epoch train_loss valid_loss time 0 456718.531250 367541.812500 00:00 1 389547.500000 367408.718750 00:00 2 386598.062500 348744.312500 00:00 3 351353.625000 326156.406250 00:00 4 335319.843750 325005.437500 00:00 And now we can calculate the MAE\npreds, targets = learn.get_preds() mae(preds, targets) tensor(365.4373)\rIt\u0026rsquo;s actually worse than the random forest, probably due to the tiny size of this dataset. Neural networks typically require large amounts of data to generalize well and learn complex patterns. That being said, since we expect the predictions from both models to be uncorrelated, it\u0026rsquo;s worth checking to see if an ensemble of both would generate a better prediction than either. Even though the random forest is already an ensemble of decision trees, it can still feed in to yet another.\nTo do so, we need to format the predictions returned by the neural network, removing the tensor unit axis, and converting to numpy array\nrf_preds = m.predict(valid_xs) nn_preds = to_np(preds.squeeze()) ens_preds = (nn_preds+rf_preds)/2 mae(ens_preds, valid_y) 3010.618897426144\r","date":"May 8, 2023","permalink":"/professional/posts/blueberry-yield/","section":"Posts","summary":"A recent Kaggle competition was launched that asks users to predict harvest yields from wild blueberries growing in the hills and mountains of the Northeast.","title":"Tabular modeling with Kaggle: predicting blueberry harvest yield"},{"content":"Pandas gains great speed from loading everything into RAM but it comes with the obvious constraint of how much has been installed. Over on Kaggle, a user who faces such a constraint has claimed to reduce his memory consumption by 70% using simple datatype conversion based on the largest and smallest numbers in each of the columns.\nWhen reading data in pandas, several memory management options exist at the outset that offer tradeoffs such as accuracy and speed:\nmemory_map: maps the file on disk rather than loading it into memory, reducing memory usage but increasing I/O time, especially if storage is not an SSD or NVME low_memory and chunksize: parses the file in chunks rather than all at once, reducing memory usage but potentially affecting performance. Additionally, the use of a small chunksize may also cause issues with data consistency or integrity, especially if the data contains inter-row dependencies. dtype: specifies the data types of columns in the resulting DataFrame, allowing for more efficient memory usage, as mentioned above Commonsense adjustments, such as only reading required columns, can also be made, but the opposite end of the spectrum\u0026ndash;distributed computing that uses arbitrary numbers of machines to execute code\u0026ndash;also exists.\nAn interesting conclusion then, is that a data source\u0026rsquo;s size doesn\u0026rsquo;t necessarily have a 1:1 relation with total memory usage, meaning a 2gb file may actually take more or less than 2gb depending on the datatypes assigned to its columns. Furthermore, the method used to measure the memory usage can also report drastically different numbers, as described here. Of particular note are strings, which can vary greatly (a string could contain a haiku or the full text of a holy book), and can be counted as pointers or the objects themselves.\n","date":"May 3, 2023","permalink":"/professional/posts/dataset-memory-compression/","section":"Posts","summary":"Pandas gains great speed from loading everything into RAM but it comes with the obvious constraint of how much has been installed.","title":"Memory compression for large datasets"},{"content":"\rRecommendation systems are one of the applications of machine learning that have become so embedded in daily life that it can be surprising to consider them as even related to machine learning. Nonetheless, looking at their internals provides a good scaffolding to use for more advanced topics.\nNetflix, Amazon, and Spotify all have suggested shows, products, and songs for their users. Though the recommended items are all different, all share the same origin of being generated by a process called \u0026lsquo;collaborative filtering\u0026rsquo;. The essence of it boils down to three steps: identify the things you used or liked, find other users who used or liked the same things, and suggest things that the other users used or liked. Notably, the process doesn\u0026rsquo;t rely on user data entry or any manual assignment of categories for recommendations. Instead, what\u0026rsquo;s happens is the attribution of latent factors to users and items. These are numerical representations of the strength of the many and varied motivations behind user ratings and selections. Similarly, the recommendations have corresponding numbers that represent how well they fit those criteria.\nData setup # To make things less abstract let\u0026rsquo;s use an example based on a dataset of boardgames from BoardGameGeek that\u0026rsquo;s been uploaded to Kaggle. Once downloaded, it provides 9 files total but let\u0026rsquo;s just use two to start with, giving us some basic data about board games and users.\nfrom fastai.collab import * from fastai.tabular.all import * from pathlib import Path # import zipfile # zipdata = zipfile.ZipFile(\u0026#39;boardgamegeek.zip\u0026#39;) # zipdata.extractall(path=\u0026#39;./data\u0026#39;) # zipdata.close() gamepath = Path(\u0026#39;./data/games.csv\u0026#39;) games = pd.read_csv(gamepath) subset = [\u0026#39;Name\u0026#39;, \u0026#39;YearPublished\u0026#39;, \u0026#39;Kickstarted\u0026#39;, \u0026#39;NumUserRatings\u0026#39;] games[subset].head() Name YearPublished Kickstarted NumUserRatings 0 Die Macher 1986 0 5354 1 Dragonmaster 1981 0 562 2 Samurai 1998 0 15146 3 Tal der Könige 1992 0 340 4 Acquire 1964 0 18655 The games.csv file contains much more metadata than we need right now so this is just a subset. As for the users, it\u0026rsquo;s nothing more than pairing game and user ids along with a rating. However, there\u0026rsquo;s enough data present that it will significantly bog us down, so we\u0026rsquo;re taking a random sample of 300000.\nusers = pd.read_csv(\u0026#39;./data/user_ratings.csv\u0026#39;, dtype={\u0026#39;Rating\u0026#39;:np.float16}).sample(300000, random_state=9000) users.head() BGGId Rating Username 3347896 124742 7.300781 Coprophage 11084381 154809 8.000000 Jonathan Degann 7903360 171623 8.500000 Odoren 11776263 266507 10.000000 ThomasArya 14497064 1270 7.898438 Ketch High level overview # How does a machine understand if you like something and by how much? First by converting the terms of the discussion into numbers. Suppose we are considering Risk, the classic game of war and conquest, with 31510 ratings in the dataset.\ngames.query(\u0026#39;Name == \u0026#34;Risk\u0026#34;\u0026#39;)[subset] Name YearPublished Kickstarted NumUserRatings 159 Risk 1959 0 31510 Users who rated it may have been considering any number of aspects they encountered while playing Risk, such as theme, playtime, complexity, and newness. Risk delivers fairly well on the theme of war, its playtime can be short as well as long, has low complexity in its rules, and is an old title. We could assign numbers between -1 and 1 to each of these like so:\nrisk = np.array([0.7,0.5,0.3,-0.6]) Similarly, a user might have a low interest in war games, be short on free time, prefers simplicity, and enjoys newer games. They could be assigned these numbers:\nuser1 = np.array([-.8,0.2,-0.5,0.6]) Collaborative filtering recommends items to users if the match between them is high, and it determines this by multiplying the arrays and adding up the result:\n(user1 * risk).sum() -0.97\rThe operation is referred to as a dot product and the arrays of numbers are the latent factors. In this case the -0.97 indicates a poor match. Someone with the opposite preferences would yield a higher number and thus be recommended Risk.\nThe latent factors in our example were arbitrarily selected (both the array lengths and array values), but in practice machine learning doesn\u0026rsquo;t start any differently\u0026ndash;the process initializes from random weights and refines them as the model learns.\nPreparing data # To start, we need to put the data into a dataloader, which is a fastai feature that helps with the creation of mini-batches for iteration during machine learning, and identify some constants: the number of users and games in the dataset. We arbitrarily pick 9 as the number of factors to train for.\nratings = users.merge(games) dls = CollabDataLoaders.from_df(ratings, item_name=\u0026#39;Name\u0026#39;, rating_name=\u0026#39;Rating\u0026#39;, user_name=\u0026#39;Username\u0026#39;) n_users = len(dls.classes[\u0026#39;Username\u0026#39;]) n_games = len(dls.classes[\u0026#39;Name\u0026#39;]) n_factors = 5 The next step is to create one-hot encodings: these are tensors that are mostly zeros except at one index, and they will represent categorical data about the boardgames, such as theme and year of release. Furthermore, it needs to be able to be passed as arguments to parameters.\ndef create_params(size): return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) The main components of the model # The model will be trained by using dot products on the users and boardgames but there are additional pieces that improve its performance.\nBias # One of the most relatable is bias. We all know people who think everything they try is the most amazing thing ever. Conversely, some people find flaws in everything. Adding additional tensors of equal size will prevent these kinds of ratings from distorting the machine learning model.\nclass BoardGameRecs(Module): def __init__(self, n_users, n_games, n_factors, y_range=(0,10.5)): self.user_factors = create_params([n_users, n_factors]) self.user_bias = create_params([n_users]) self.game_factors = create_params([n_games, n_factors]) self.game_bias = create_params([n_games]) self.y_range = y_range def forward(self, x): users = self.user_factors[x[:,0]] games = self.game_factors[x[:,1]] res = (users * games).sum(dim=1) res += self.user_bias[x[:,0]] + self.game_bias[x[:,1]] return sigmoid_range(res, *self.y_range) Weight decay # Weight decay is modification of the loss function every time it is calculated, simply adding a large constant, the intent of which is to counteract overfitting. Making the loss function grow bigger is counterproductive in the short run, since it extends training time, but the tradeoff is worth it.\nBut how does simple addition prevent overfitting? Consider a plot of an overfitted loss function:\nimport seaborn as sns import matplotlib.pyplot as plt sns.relplot(games, x=\u0026#39;AvgRating\u0026#39;, y=\u0026#39;NumUserRatings\u0026#39;, alpha=.5) plt.plot([1, 10], [0, 77000], label=\u0026#39;under fit\u0026#39;, color=\u0026#39;navy\u0026#39;) plt.plot(range(1,11), [13, 300, 4000, 100, 8000, 2000, 51000, 110000, 700, 123], label=\u0026#39;over fit\u0026#39;, color=\u0026#39;red\u0026#39;, scaley=\u0026#39;log\u0026#39;) plt.legend(loc=\u0026#34;upper left\u0026#34;) plt.show() When models are underfit, the loss function can take a wildly inaccurate and straight path. Overfit models, on the other hand, tend to zigzag as they try to adhere to data points. The scatterplot above is from some of the omitted columns in the boardgame dataset that\u0026rsquo;s been overlaid with cherry-picked numbers to illustrate, but this example also shows the zigzag pattern: Next, consider that quadratics such as \\( ax^2 + bx \\) get steeper and narrower as \\( a \\) and \\( b \\) grow larger. Weight decay leverages this effect so that the resulting trained weights (which want to go in the opposite direction of this line) do not overfit.\nWhat is forward and x? # The forward function allows pytorch to send arguments to other method calls. The model input is a tensor of shape [batch_size, 2], where the first column is user ids (x[:,0]) and the second column game ids (x[:,1]).\nResults # Now we can look at some training results at 5 epochs.\nmodel = BoardGameRecs(n_users, n_games, n_factors) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.5) epoch train_loss valid_loss time 0 3.812454 3.783754 00:23 1 3.866644 3.770411 00:22 2 3.720784 3.758823 00:22 3 3.837165 3.735405 00:22 4 3.676429 3.727141 00:22 When we discussed the concept of biases for machine learning, it was with users as an example, but it applies to the boardgames in our dataset as well. Instead of perpetual critical reviews or excess praise, we have consistently high ratings even if the user isn\u0026rsquo;t supposed to be a good match for the game, and consistently low ratings even if the user should like it.\ngame_bias = learn.model.game_bias.squeeze() idxs = game_bias.argsort()[:5] [dls.classes[\u0026#39;Name\u0026#39;][i] for i in idxs] ['Candy Land',\r'Monopoly',\r'Tic-Tac-Toe',\r'Chutes and Ladders',\r'The Game of Life']\rHere we see some classic boardgames that have fallen out of favor with BoardGameGeek users: Tic-Tac-Toe, Chutes and Ladders, Candy Land. Even users who prefer simple and light boardgames want to play alternatives to these.\nidxs = game_bias.argsort(descending=True)[:5] [dls.classes[\u0026#39;Name\u0026#39;][i] for i in idxs] ['Terraforming Mars',\r'7 Wonders Duel',\r'Scythe',\r'Gloomhaven',\r'The Castles of Burgundy']\rAnd at the other end of the spectrum are 7 Wonders and Terraforming Mars, representing the boardgame equivalent of blockbuster movies. These are well-liked by all users on BoardGameGeek even if they don\u0026rsquo;t typically enjoy the genres that each belong to.\nThis is a preview of a future post but warrants showing here since computing cycles have already been used to train the model. The following chart uses two of the model\u0026rsquo;s factors as axes, to which are ploted some of the highest rated boardgames.\nThere are clusters that have formed organically just from user ratings and their positions on the plot represent potential quadrants relating to the boardgame characteristics or themes.\nFurther reading # A great post on the broader context of fitting categorical data into machine learning models, which the one-hot encoding used here is a part of, can be found here: https://www.featureform.com/post/the-definitive-guide-to-embeddings\n","date":"Apr 28, 2023","permalink":"/professional/posts/collaborative-filtering/","section":"Posts","summary":"Recommendation systems are one of the applications of machine learning that have become so embedded in daily life that it can be surprising to consider them as even related to machine learning.","title":"Recommendation systems: exploring collaborative filtering with boardgame ratings"},{"content":"","date":"Apr 21, 2023","permalink":"/professional/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"\rIntro # Tensors are the foundational building blocks of machine learning and their nuances are worth spending some time on. This post is meant to be a refresher on pytorch tensors, pulling together information from various sources.\nCreating tensors # import torch import math x = torch.empty(3,4) print(type(x)) print(x) \u0026lt;class 'torch.Tensor'\u0026gt;\rtensor([[0., 0., 0., 0.],\r[0., 0., 0., 0.],\r[0., 0., 0., 0.]])\rTo start off, the pytorch docs include a great primer that begins by initializing a tensor just like the one above: \u0026gt; Let\u0026rsquo;s unpack what we just did: \u0026gt; - We created a tensor using one of the numerous factory methods attached to the torch module. \u0026gt; - The tensor itself is 2-dimensional, having 3 rows and 4 columns. \u0026gt; - The type of the object returned is torch.Tensor, which is an alias for torch.FloatTensor; by default, PyTorch tensors are populated with 32-bit floating point numbers. (More on data types below.) \u0026gt; - You will probably see some random-looking values when printing your tensor. The torch.empty() call allocates memory for the tensor, but does not initialize it with any values - so what you\u0026rsquo;re seeing is whatever was in memory at the time of allocation.\nHowever, uncontrolled random tensor starting values are often less useful than all zeros or ones. If randomness is desired, it can be fixed to be the same every time using manual_seed:\nzeros = torch.zeros(2, 3) print(zeros) ones = torch.ones(2, 3) print(ones) torch.manual_seed(1729) random = torch.rand(2, 3) print(random) tensor([[0., 0., 0.],\r[0., 0., 0.]])\rtensor([[1., 1., 1.],\r[1., 1., 1.]])\rtensor([[0.3126, 0.3791, 0.3087],\r[0.0736, 0.4216, 0.0691]])\rComparison with numpy arrays # Tensors are higher dimensional analogues to numpy arrays, sharing many qualities such as their objectives, apis, syntax, and functionality. Notable differences do exist however:\ntensors have specialized gpu processing support to accelerate operations except for broadcasting operations, tensors cannot be \u0026lsquo;jagged\u0026rsquo;, meaning having dimensions of different length all data in a tensor must be the same type Runtime and syntax errors will appear if these restrictions are ignored x + zeros RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1 import numpy as np np.array([1, \u0026#39;two\u0026#39;, 3]) array(['1', 'two', '3'], dtype='\u0026lt;U11')\rtorch.rand([1, \u0026#39;two\u0026#39;, 3]) TypeError: rand(): argument \u0026#39;size\u0026#39; must be tuple of ints, but found element of type str at pos 2 Going beyond 3d # Higher dimensionality is difficult to visualize and often invites people to start drawing volumes and hypercubes\u0026ndash;a consequence of \u0026lsquo;dimension\u0026rsquo; in common usage as well as a sign of how far the cartesian system has permeated. How do you think beyond 3D? We can go \u0026lsquo;back to 2D\u0026rsquo;, in a sense. Forget about trying to imagine the data in shapes, connecting lines between them, and instead consider tensor dimensionaliy simply as arrays of arrays (or lists of lists).\nObserve how the output grows when we create tensors of increasing size:\ntorch.ones(2,2) tensor([[1., 1.],\r[1., 1.]])\rtorch.ones(2,2,2) tensor([[[1., 1.],\r[1., 1.]],\r[[1., 1.],\r[1., 1.]]])\rtorch.ones(2,2,2,2) tensor([[[[1., 1.],\r[1., 1.]],\r[[1., 1.],\r[1., 1.]]],\r[[[1., 1.],\r[1., 1.]],\r[[1., 1.],\r[1., 1.]]]])\rEach additional number passed to torch.ones is an additional dimension, so we\u0026rsquo;re currently at 4 dimensions. There are a few things to note from the pattern that emerges from playing around with this. These examples kept all the arguments identical but modifying the last one makes it clearer:\ntorch.ones(2,3,1,5) tensor([[[[1., 1., 1., 1., 1.]],\r[[1., 1., 1., 1., 1.]],\r[[1., 1., 1., 1., 1.]]],\r[[[1., 1., 1., 1., 1.]],\r[[1., 1., 1., 1., 1.]],\r[[1., 1., 1., 1., 1.]]]])\rthe first number determines the number of groups at the highest level the last number determines the size of the innermost elements of the tensor (here, 5) this is the same as how long each printed line is in the output for each number between the first and the last, groups are recursively nested ","date":"Apr 11, 2023","permalink":"/professional/posts/tensor-refresher/","section":"Posts","summary":"Intro # Tensors are the foundational building blocks of machine learning and their nuances are worth spending some time on.","title":"Tensor Refresher"},{"content":"","date":"Jan 1, 0001","permalink":"/professional/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"Jan 1, 0001","permalink":"/professional/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"Jan 1, 0001","permalink":"/professional/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"Jan 1, 0001","permalink":"/professional/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"This is a small blog testing out static site generation using Hugo\n","date":"Jan 1, 0001","permalink":"/professional/about/","section":"luminal transubstantiation","summary":"This is a small blog testing out static site generation using Hugo","title":"Tony Tran"}]