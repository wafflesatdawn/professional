[{"content":"","date":"May 8, 2023","permalink":"/professional/","section":"luminal transubstantiation","summary":"","title":"luminal transubstantiation"},{"content":"\rA recent Kaggle competition was launched that asks users to predict harvest yields from wild blueberries growing in the hills and mountains of the Northeast. As you might imagine, both the harvest and the subsequent harvest of data are not easy to come by. The dataset is actually generated from synthetic data which is itself based on a simulation of harvests that\u0026rsquo;s been refined for 30 years. Background details aside, the dataset will be a good example to use for tabular modeling, where the objective is to predict one column\u0026rsquo;s values based on values from the others. Tabular modeling includes decision trees and their evolved form, random forests, both of which will be focused on here using scikit-learn.\nAlthough tabular modeling is not as attention-grabbing as NLP and image recognition, it offers many strengths in its own right for structured data:\nfaster training ease of interpretation unbound by hardware constraints at scale (eg: tensor cores) less hyperparameter tuning That said, there are two exceptions to this: high cardinality in important categorical variables presence of data that would be handled much better by neural networks, such as columns containing plain text Unstructured data will always be the domain of deep learning, but for structured data the contest is less clear and it\u0026rsquo;s worthwhile to consider tabular modeling\u0026ndash;at the very least as a baseline.\nSetting up data # from pathlib import Path from pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype from fastai.tabular.all import * from sklearn.ensemble import RandomForestRegressor from sklearn.tree import DecisionTreeRegressor, export_graphviz import dtreeviz from IPython.display import Image, display_svg, SVG import warnings import seaborn as sns # suppress the warning message from sklearn warnings.filterwarnings(action=\u0026#39;ignore\u0026#39;, category=UserWarning) data_path = Path(\u0026#39;./data\u0026#39;) df = pd.read_csv(data_path/\u0026#39;train.csv\u0026#39;, low_memory=False) As described by Kaggle, this dataset is very small\u0026ndash;barely over a megabyte\u0026ndash;and is intended to be a lighter complement to its intenser options. Here we see just a handful of columns, many of which are related, and less than 50000 rows across both the training and test sets.\nData prep # [{df[i].name:df[i].unique()[:10]} for i in df.columns] [{'id': array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9], dtype=int64)},\r{'clonesize': array([25. , 12.5, 37.5, 20. , 10. , 40. ])},\r{'honeybee': array([ 0.5 , 0.25 , 0.75 , 0.537, 0. , 18.43 , 6.64 ])},\r{'bumbles': array([0.25 , 0.38 , 0.117, 0.058, 0.56 , 0.065, 0. , 0.585, 0.042,\r0.293])},\r{'andrena': array([0.75 , 0.5 , 0.63 , 0.38 , 0.25 , 0.409, 0.707, 0. , 0.24 ,\r0.56 ])},\r{'osmia': array([0.5 , 0.63 , 0.75 , 0.25 , 0.38 , 0.058, 0.117, 0.62 , 0.585,\r0. ])},\r{'MaxOfUpperTRange': array([69.7, 86. , 77.4, 94.6, 89. , 79. ])},\r{'MinOfUpperTRange': array([42.1, 52. , 46.8, 57.2, 39. ])},\r{'AverageOfUpperTRange': array([58.2, 71.9, 64.7, 79. , 65.6])},\r{'MaxOfLowerTRange': array([50.2, 62. , 55.8, 68.2, 66. , 52. ])},\r{'MinOfLowerTRange': array([24.3, 30. , 27. , 33. , 28. , 25. , 31. ])},\r{'AverageOfLowerTRange': array([41.2, 50.8, 45.8, 55.9, 45.3])},\r{'RainingDays': array([24. , 34. , 1. , 16. , 3.77, 26. ])},\r{'AverageRainingDays': array([0.39, 0.56, 0.1 , 0.26, 0.06, 0.25, 0.07, 0.14])},\r{'fruitset': array([0.4250109 , 0.44490828, 0.55292683, 0.56597648, 0.57967664,\r0.56523939, 0.49873 , 0.61988773, 0.53255682, 0.34006335])},\r{'fruitmass': array([0.41754541, 0.42205139, 0.47085288, 0.47813655, 0.49416475,\r0.4843495 , 0.44219309, 0.52950157, 0.46536689, 0.38176787])},\r{'seeds': array([32.46088718, 33.85831713, 38.34178123, 39.46756134, 40.48451183,\r40.55501923, 35.51753863, 42.19101338, 36.16604354, 28.76356526])},\r{'yield': array([4476.81146, 5548.12201, 6869.7776 , 6880.7759 , 7479.93417,\r7267.28344, 5739.68029, 7920.06175, 6465.37205, 3519.43131])}]\rBased on how few unique values they contain, the first five columns might warrant treatment as categorical variables, so we can mark them as such\ndf.clonesize = df.clonesize.astype(\u0026#39;category\u0026#39;) df.honeybee = df.honeybee.astype(\u0026#39;category\u0026#39;) df.bumbles = df.bumbles.astype(\u0026#39;category\u0026#39;) df.andrena = df.andrena.astype(\u0026#39;category\u0026#39;) df.osmia = df.osmia.astype(\u0026#39;category\u0026#39;) The dependent variable we are predicting is yield, and procs are wrappers on Pandas that handle strings and missing data. This dataset does not contain strings bu the functionality are grouped together. Categorify is a TabularProc that replaces a column with a numeric categorical column. FillMissing is a TabularProc that replaces missing values with the median of the column, and creates a new Boolean column that is set to True for any row where the value was missing.\ndep_var = \u0026#39;yield\u0026#39; procs = [Categorify, FillMissing] Based on the structure of the dataset, we will randomly split the data into train and validation sets\nrng = np.random.default_rng() np.random.seed(11) train_size = round(len(df) * .7) train_idx = rng.integers(low=0, high=df.last_valid_index(), size=train_size) splits = (list(train_idx), list(df.index[~train_idx])) Tell TabularPandas which columns are continuous and categorical. Save the processed data for later use.\ncont,cat = cont_cat_split(df, 1, dep_var=dep_var) to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) save_pickle(data_path/\u0026#39;to.pkl\u0026#39;,to) Creating decision trees # First define x and y, the independent and dependent variables. Then create the decision tree.\nto = load_pickle(data_path/\u0026#39;to.pkl\u0026#39;) trn_xs,trn_y = to.train.xs,to.train.y valid_xs,valid_y = to.valid.xs,to.valid.y m = DecisionTreeRegressor(max_leaf_nodes=4) m.fit(trn_xs, trn_y); Visualization # This function visualizes the decision tree for the training x data that\u0026rsquo;s been passed to the tabularpandas. The first node is before anything has been done. The value is the mean of the variable we\u0026rsquo;re trying to predict, yield, and the sample is the length of the dataframe.\nimport graphviz def draw_tree(t, df, size=10, ratio=0.6, precision=2, **kwargs): s=export_graphviz(t, out_file=None, feature_names=df.columns, filled=True, rounded=True, special_characters=True, rotate=False, precision=precision, **kwargs) return graphviz.Source(re.sub(\u0026#39;Tree {\u0026#39;, f\u0026#39;Tree {{ size={size}; ratio={ratio}\u0026#39;, s)) draw_tree(m, trn_xs, size=10) The next two nodes come from bisecting the dataset by values for fruitset above and below 0.5.\nAlternative visualization using dtreeviz, showing the distribution of data along with the bisecting lines\ndtreeviz.model(m, X_train=trn_xs, y_train=trn_y, feature_names=df.columns, target_name=dep_var).view() These previews were limited to just 4 nodes but now we will remove it\nm = DecisionTreeRegressor() m.fit(trn_xs, trn_y); Performance evaluation # The Kaggle competition that this data was taken from will evaluate submissions based on mean absolute error\n$$ MAE = \\frac{1}{n}\\sum\\limits_{i=1}^n{|x_i - y_i|} $$\nwhere each \\( x_i \\) represents the predicted target, \\( y_i \\) represents the ground truth, and \\( n \\) is the number of rows in the test set.\ndef mae(predictions, actuals): \u0026#34;\u0026#34;\u0026#34;calculate the mean absolute error between prediction and actual values Args: predictions (Series): from training set actuals (Series): from validation set Returns: _type_: float \u0026#34;\u0026#34;\u0026#34; return abs(predictions - actuals).sum() / len(predictions) Generating our predictions from the model and taking the MAE:\npredictions = m.predict(trn_xs) mae(predictions, trn_y) 0.0\rChecking it against our validation set:\nmae(m.predict(valid_xs), valid_y) 267.3668955897554\rA mean absolute error of 0 indicates overfitting, as the default setting for sklearn is to continue splitting nodes until they run out. The total nodes, or leaves, in the tree is almost as high as the total rows in the training set:\nm.get_n_leaves(), len(trn_xs) (7346, 10702)\rChanging it to 25 modes will fix the problem, bringing the MAE closer to the validation set.\nm = DecisionTreeRegressor(min_samples_leaf=25) m.fit(trn_xs, trn_y) mae(m.predict(trn_xs), trn_y) 323.21402610989696\rTrees become forest # Decision trees offer a balance of generalization and accuracy, but they are on opposite ends of a fulcrum. Limiting the size of the tree means it generalizes well at the expense of accuracy and vice versa. To overcome this compromise, data scientists started using a new technique called random forests, extending the analogy. The intuition behind random forests echoes the central limit theorem: an aggregated measure derived from several samples is more accurate than any of the individual samples. However, random forests has specific criteria\nsubset and bootstrap data from the training set randomly use different subsets of columns when choosing splits in each decision tree Creating a random forest # The setup will be similar to the decision trees from earlier, specifying some of the same limits\ndef rf(xs, y, n_estimators=40, max_samples=2000, max_features=0.5, min_samples_leaf=5, **kwargs): \u0026#34;\u0026#34;\u0026#34;generate a random forest Args: xs (DataFrame): independent variables y (Series): dependent variable n_estimators (int, optional): number of trees. Defaults to 40. max_samples (_type_, optional): rows to sample for training each tree. Defaults to 2000. max_features (float, optional): number of features to consider when looking for the best split. Defaults to 0.5, meaning half. min_samples_leaf (int, optional): minimum number of samples in each leaf. Defaults to 5. Returns: _type_: _description_ \u0026#34;\u0026#34;\u0026#34; return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) Comparing the training results to the validation set, MAE using random forest gains a slight improvement over the single large decision tree\nm = rf(trn_xs, trn_y) mae(m.predict(trn_xs), trn_y), mae(m.predict(valid_xs), valid_y) (309.1868055637817, 341.3091030297672)\rFurther inspection of random forest performance # The random forest we created created has 40 trees, each of which was can be accessed by indexing from m.estimators_, which returns a list of all the predicted yields\nm.estimators_[3].predict(valid_xs) array([4662.13269043, 4161.40205078, 3451.59905134, ..., 5439.28586155,\r5566.04943848, 7025.54951172])\rNumpy\u0026rsquo;s stack method allows for easy manipulations of arrays, quickly moving values from one to another as seen its documentation examples:\n\u0026gt;\u0026gt;\u0026gt; arrays = [np.random.randn(3, 4) for _ in range(10)] \u0026gt;\u0026gt;\u0026gt; np.stack(arrays, axis=0).shape (10, 3, 4) \u0026gt;\u0026gt;\u0026gt; np.stack(arrays, axis=1).shape (3, 10, 4) \u0026gt;\u0026gt;\u0026gt; np.stack(arrays, axis=2).shape (3, 4, 10) \u0026gt;\u0026gt;\u0026gt; a = np.array([1, 2, 3]) \u0026gt;\u0026gt;\u0026gt; b = np.array([4, 5, 6]) \u0026gt;\u0026gt;\u0026gt; np.stack((a, b)) array([[1, 2, 3], [4, 5, 6]]) \u0026gt;\u0026gt;\u0026gt; np.stack((a, b), axis=-1) array([[1, 4], [2, 5], [3, 6]]) The resulting output from stack is another numpy array, giving us access to the mean method. Usually taking the mean of an array reduces the dimensions and returns just a scalar. However, passing an optional argument will instead take the mean along either axis.\npreds = np.stack([t.predict(valid_xs) for t in m.estimators_]) len(trn_xs) == len(preds.mean(0)) True\rUsing this gives back the predicted yields for each decision tree (axis 1) or the entire dataset (axis 0), which will return the same MAE from as earlier\nmae(preds.mean(0), valid_y) 341.3091030297672\rThis can then be used to plot how the MAE exponentially improves as more decision trees get added, starting from 0 all the way to 40, the maximun number that we specified.\nplt.plot([mae(preds[:i+1].mean(0), valid_y) for i in range(40)]) The model also offers oob_prediction_, which is like a miniature validation set. Because each tree in a random forest trains on a different subset of data, all data outside each subset is available to use in checking how well it generalizes.\nmae(m.oob_prediction_, trn_y) 334.4666069531408\rModel interpretation # Using the same stack from above lets us look at the random forest in greater detail. As a reminder, it has as many rows as there are trees in the model, and each row contains the subset of data available to each tree\npreds.shape (40, 10702)\rTo start off, since each decision tree is independent of the others, how do their predictions vary?\nStandard deviation # We can calculate the standard deviation for each tree along axis 0, as with the mean earlier\npreds_std = preds.std(0) sns.displot(preds_std) The distribution is reasonably narrow for most of the data but a long right tail goes as high as 3-4 times the median.\nImmportance of features # Next, we can take advantage of the feature_importances_ attribute from sklearn. (Side note: there is considerable looseness of usage regarding machine learning jargon such that the library includes its own glossary amongst other glossaries created by others.) It provides the importance that the model assigns to each feature of the dataset.\ndef rf_feat_importance(m, df): return pd.DataFrame({\u0026#39;cols\u0026#39;:df.columns, \u0026#39;imp\u0026#39;:m.feature_importances_} ).sort_values(\u0026#39;imp\u0026#39;, ascending=False) fi = rf_feat_importance(m, trn_xs) sns.barplot(fi, x=\u0026#39;imp\u0026#39;, y=\u0026#39;cols\u0026#39;, orient=\u0026#39;h\u0026#39;) \u0026lt;AxesSubplot: xlabel='imp', ylabel='cols'\u0026gt;\rThe model seems to think that only 3 things are important for prediction yield, and by a large margin\nLow importance variables # Considering the huge disparity in importance, we should try retraining the model with just a subset, perhaps the top three.\ntop3 = fi.nlargest(3, \u0026#39;imp\u0026#39;).cols trn_xs_imp = trn_xs[top3] valid_xs_imp = valid_xs[top3] m = rf(trn_xs_imp, trn_y) mae(m.predict(trn_xs_imp), trn_y) 323.4433739666308\rMAE is slightly worse, but in a dataset with significantly more columns, we would have good reason to narrow our analysis of factors using the quantified importance as cutoff criteria. Here we will decide to keep the extra columns in exchange for the better score.\nFinal predictions # To submit to Kaggle, all we need to do is load the test data and feed it to our random forest model, making sure the order and number of columns is consistent between both datasets. To do that, we need to repeat the relevant data processing steps and redefine the model.\nm = rf(trn_xs, trn_y) df_test = pd.read_csv(data_path/\u0026#39;test.csv\u0026#39;, low_memory=False) test_to = TabularPandas(df_test, procs, cat, cont) final_preds = m.predict(test_to[trn_xs.columns]) submission = pd.DataFrame() submission[\u0026#39;yield\u0026#39;] = final_preds submission.index += 15289 submission.to_csv(\u0026#34;submission.csv\u0026#34;, index=True, header=True, index_label=\u0026#34;id\u0026#34;) Limitations of tabular modeling # Due to the fundamental design of random forests, they are bad at extrapolating. Because a random forest averages the predictions of its trees, each of which average the the values in their leaves, predictions will never go beyond the range of values in the training data. The good news is that the problem can be mitigated by checking whether our model\u0026rsquo;s predictive power hinges on conditions that aren\u0026rsquo;t shared between the training and validation sets.\nTo achieve this we\u0026rsquo;ll simply create a new dependent variable that marks whether data is from the training set or the validation set and use a random forest to predict it.\ncombo = pd.concat([trn_xs, valid_xs]) is_valid = np.array([0]*len(trn_xs) + [1]*len(valid_xs)) m = rf(combo, is_valid) rf_feat_importance(m, combo)[:5] cols imp 5 id 0.203816 14 fruitset 0.181433 15 fruitmass 0.178354 16 seeds 0.172282 3 andrena 0.052358 We then look at the feature importance of the new model. For each feature, the higher the importance, the greater the disparity between validation and training datasets. In this case, there\u0026rsquo;s not much of a discrepancy betweeen validation and training data, with the highest importance at 0.2. This result reflects the random splitting of the original data, but if it were structured differently, such as a time series, then random splitting would not have been the ideal choice, leading to greater chance of bigger discrepancies.\n","date":"May 8, 2023","permalink":"/professional/posts/blueberry-yield/","section":"Posts","summary":"A recent Kaggle competition was launched that asks users to predict harvest yields from wild blueberries growing in the hills and mountains of the Northeast.","title":"Tabular modeling with Kaggle: predicting blueberry harvest yield"},{"content":"Pandas gains great speed from loading everything into RAM but it comes with the obvious constraint of how much has been installed. Over on Kaggle, a user who faces such a constraint has claimed to reduce his memory consumption by 70% using simple datatype conversion based on the largest and smallest numbers in each of the columns.\nWhen reading data in pandas, several memory management options exist at the outset that offer tradeoffs such as accuracy and speed:\nmemory_map: maps the file on disk rather than loading it into memory, reducing memory usage but increasing I/O time, especially if storage is not an SSD or NVME low_memory and chunksize: parses the file in chunks rather than all at once, reducing memory usage but potentially affecting performance. Additionally, the use of a small chunksize may also cause issues with data consistency or integrity, especially if the data contains inter-row dependencies. dtype: specifies the data types of columns in the resulting DataFrame, allowing for more efficient memory usage, as mentioned above Commonsense adjustments, such as only reading required columns, can also be made, but the opposite end of the spectrum\u0026ndash;distributed computing that uses arbitrary numbers of machines to execute code\u0026ndash;also exists.\nAn interesting conclusion then, is that a data source\u0026rsquo;s size doesn\u0026rsquo;t necessarily have a 1:1 relation with total memory usage, meaning a 2gb file may actually take more or less than 2gb depending on the datatypes assigned to its columns. Furthermore, the method used to measure the memory usage can also report drastically different numbers, as described here. Of particular note are strings, which can vary greatly (a string could contain a haiku or the full text of a holy book), and can be counted as pointers or the objects themselves.\n","date":"May 3, 2023","permalink":"/professional/posts/dataset-memory-compression/","section":"Posts","summary":"Pandas gains great speed from loading everything into RAM but it comes with the obvious constraint of how much has been installed.","title":"Memory compression for large datasets"},{"content":"\rRecommendation systems are one of the applications of machine learning that have become so embedded in daily life that it can be surprising to consider them as even related to machine learning. Nonetheless, looking at their internals provides a good scaffolding to use for more advanced topics.\nNetflix, Amazon, and Spotify all have suggested shows, products, and songs for their users. Though the recommended items are all different, all share the same origin of being generated by a process called \u0026lsquo;collaborative filtering\u0026rsquo;. The essence of it boils down to three steps: identify the things you used or liked, find other users who used or liked the same things, and suggest things that the other users used or liked. Notably, the process doesn\u0026rsquo;t rely on user data entry or any manual assignment of categories for recommendations. Instead, what\u0026rsquo;s happens is the attribution of latent factors to users and items. These are numerical representations of the strength of the many and varied motivations behind user ratings and selections. Similarly, the recommendations have corresponding numbers that represent how well they fit those criteria.\nData setup # To make things less abstract let\u0026rsquo;s use an example based on a dataset of boardgames from BoardGameGeek that\u0026rsquo;s been uploaded to Kaggle. Once downloaded, it provides 9 files total but let\u0026rsquo;s just use two to start with, giving us some basic data about board games and users.\nfrom fastai.collab import * from fastai.tabular.all import * from pathlib import Path # import zipfile # zipdata = zipfile.ZipFile(\u0026#39;boardgamegeek.zip\u0026#39;) # zipdata.extractall(path=\u0026#39;./data\u0026#39;) # zipdata.close() gamepath = Path(\u0026#39;./data/games.csv\u0026#39;) games = pd.read_csv(gamepath) subset = [\u0026#39;Name\u0026#39;, \u0026#39;YearPublished\u0026#39;, \u0026#39;Kickstarted\u0026#39;, \u0026#39;NumUserRatings\u0026#39;] games[subset].head() Name YearPublished Kickstarted NumUserRatings 0 Die Macher 1986 0 5354 1 Dragonmaster 1981 0 562 2 Samurai 1998 0 15146 3 Tal der KÃ¶nige 1992 0 340 4 Acquire 1964 0 18655 The games.csv file contains much more metadata than we need right now so this is just a subset. As for the users, it\u0026rsquo;s nothing more than pairing game and user ids along with a rating. However, there\u0026rsquo;s enough data present that it will significantly bog us down, so we\u0026rsquo;re taking a random sample of 300000.\nusers = pd.read_csv(\u0026#39;./data/user_ratings.csv\u0026#39;, dtype={\u0026#39;Rating\u0026#39;:np.float16}).sample(300000, random_state=9000) users.head() BGGId Rating Username 3347896 124742 7.300781 Coprophage 11084381 154809 8.000000 Jonathan Degann 7903360 171623 8.500000 Odoren 11776263 266507 10.000000 ThomasArya 14497064 1270 7.898438 Ketch High level overview # How does a machine understand if you like something and by how much? First by converting the terms of the discussion into numbers. Suppose we are considering Risk, the classic game of war and conquest, with 31510 ratings in the dataset.\ngames.query(\u0026#39;Name == \u0026#34;Risk\u0026#34;\u0026#39;)[subset] Name YearPublished Kickstarted NumUserRatings 159 Risk 1959 0 31510 Users who rated it may have been considering any number of aspects they encountered while playing Risk, such as theme, playtime, complexity, and newness. Risk delivers fairly well on the theme of war, its playtime can be short as well as long, has low complexity in its rules, and is an old title. We could assign numbers between -1 and 1 to each of these like so:\nrisk = np.array([0.7,0.5,0.3,-0.6]) Similarly, a user might have a low interest in war games, be short on free time, prefers simplicity, and enjoys newer games. They could be assigned these numbers:\nuser1 = np.array([-.8,0.2,-0.5,0.6]) Collaborative filtering recommends items to users if the match between them is high, and it determines this by multiplying the arrays and adding up the result:\n(user1 * risk).sum() -0.97\rThe operation is referred to as a dot product and the arrays of numbers are the latent factors. In this case the -0.97 indicates a poor match. Someone with the opposite preferences would yield a higher number and thus be recommended Risk.\nThe latent factors in our example were arbitrarily selected (both the array lengths and array values), but in practice machine learning doesn\u0026rsquo;t start any differently\u0026ndash;the process initializes from random weights and refines them as the model learns.\nPreparing data # To start, we need to put the data into a dataloader, which is a fastai feature that helps with the creation of mini-batches for iteration during machine learning, and identify some constants: the number of users and games in the dataset. We arbitrarily pick 9 as the number of factors to train for.\nratings = users.merge(games) dls = CollabDataLoaders.from_df(ratings, item_name=\u0026#39;Name\u0026#39;, rating_name=\u0026#39;Rating\u0026#39;, user_name=\u0026#39;Username\u0026#39;) n_users = len(dls.classes[\u0026#39;Username\u0026#39;]) n_games = len(dls.classes[\u0026#39;Name\u0026#39;]) n_factors = 5 The next step is to create one-hot encodings: these are tensors that are mostly zeros except at one index, and they will represent categorical data about the boardgames, such as theme and year of release. Furthermore, it needs to be able to be passed as arguments to parameters.\ndef create_params(size): return nn.Parameter(torch.zeros(*size).normal_(0, 0.01)) The main components of the model # The model will be trained by using dot products on the users and boardgames but there are additional pieces that improve its performance.\nBias # One of the most relatable is bias. We all know people who think everything they try is the most amazing thing ever. Conversely, some people find flaws in everything. Adding additional tensors of equal size will prevent these kinds of ratings from distorting the machine learning model.\nclass BoardGameRecs(Module): def __init__(self, n_users, n_games, n_factors, y_range=(0,10.5)): self.user_factors = create_params([n_users, n_factors]) self.user_bias = create_params([n_users]) self.game_factors = create_params([n_games, n_factors]) self.game_bias = create_params([n_games]) self.y_range = y_range def forward(self, x): users = self.user_factors[x[:,0]] games = self.game_factors[x[:,1]] res = (users * games).sum(dim=1) res += self.user_bias[x[:,0]] + self.game_bias[x[:,1]] return sigmoid_range(res, *self.y_range) Weight decay # Weight decay is modification of the loss function every time it is calculated, simply adding a large constant, the intent of which is to counteract overfitting. Making the loss function grow bigger is counterproductive in the short run, since it extends training time, but the tradeoff is worth it.\nBut how does simple addition prevent overfitting? Consider a plot of an overfitted loss function:\nimport seaborn as sns import matplotlib.pyplot as plt sns.relplot(games, x=\u0026#39;AvgRating\u0026#39;, y=\u0026#39;NumUserRatings\u0026#39;, alpha=.5) plt.plot([1, 10], [0, 77000], label=\u0026#39;under fit\u0026#39;, color=\u0026#39;navy\u0026#39;) plt.plot(range(1,11), [13, 300, 4000, 100, 8000, 2000, 51000, 110000, 700, 123], label=\u0026#39;over fit\u0026#39;, color=\u0026#39;red\u0026#39;, scaley=\u0026#39;log\u0026#39;) plt.legend(loc=\u0026#34;upper left\u0026#34;) plt.show() When models are underfit, the loss function can take a wildly inaccurate and straight path. Overfit models, on the other hand, tend to zigzag as they try to adhere to data points. The scatterplot above is from some of the omitted columns in the boardgame dataset that\u0026rsquo;s been overlaid with cherry-picked numbers to illustrate, but this example also shows the zigzag pattern: Next, consider that quadratics such as \\( ax^2 + bx \\) get steeper and narrower as \\( a \\) and \\( b \\) grow larger. Weight decay leverages this effect so that the resulting trained weights (which want to go in the opposite direction of this line) do not overfit.\nWhat is forward and x? # The forward function allows pytorch to send arguments to other method calls. The model input is a tensor of shape [batch_size, 2], where the first column is user ids (x[:,0]) and the second column game ids (x[:,1]).\nResults # Now we can look at some training results at 5 epochs.\nmodel = BoardGameRecs(n_users, n_games, n_factors) learn = Learner(dls, model, loss_func=MSELossFlat()) learn.fit_one_cycle(5, 5e-3, wd=0.5) epoch train_loss valid_loss time 0 3.812454 3.783754 00:23 1 3.866644 3.770411 00:22 2 3.720784 3.758823 00:22 3 3.837165 3.735405 00:22 4 3.676429 3.727141 00:22 When we discussed the concept of biases for machine learning, it was with users as an example, but it applies to the boardgames in our dataset as well. Instead of perpetual critical reviews or excess praise, we have consistently high ratings even if the user isn\u0026rsquo;t supposed to be a good match for the game, and consistently low ratings even if the user should like it.\ngame_bias = learn.model.game_bias.squeeze() idxs = game_bias.argsort()[:5] [dls.classes[\u0026#39;Name\u0026#39;][i] for i in idxs] ['Candy Land',\r'Monopoly',\r'Tic-Tac-Toe',\r'Chutes and Ladders',\r'The Game of Life']\rHere we see some classic boardgames that have fallen out of favor with BoardGameGeek users: Tic-Tac-Toe, Chutes and Ladders, Candy Land. Even users who prefer simple and light boardgames want to play alternatives to these.\nidxs = game_bias.argsort(descending=True)[:5] [dls.classes[\u0026#39;Name\u0026#39;][i] for i in idxs] ['Terraforming Mars',\r'7 Wonders Duel',\r'Scythe',\r'Gloomhaven',\r'The Castles of Burgundy']\rAnd at the other end of the spectrum are 7 Wonders and Terraforming Mars, representing the boardgame equivalent of blockbuster movies. These are well-liked by all users on BoardGameGeek even if they don\u0026rsquo;t typically enjoy the genres that each belong to.\nThis is a preview of a future post but warrants showing here since computing cycles have already been used to train the model. The following chart uses two of the model\u0026rsquo;s factors as axes, to which are ploted some of the highest rated boardgames.\nThere are clusters that have formed organically just from user ratings and their positions on the plot represent potential quadrants relating to the boardgame characteristics or themes.\nFurther reading # A great post on the broader context of fitting categorical data into machine learning models, which the one-hot encoding used here is a part of, can be found here: https://www.featureform.com/post/the-definitive-guide-to-embeddings\n","date":"Apr 28, 2023","permalink":"/professional/posts/collaborative-filtering/","section":"Posts","summary":"Recommendation systems are one of the applications of machine learning that have become so embedded in daily life that it can be surprising to consider them as even related to machine learning.","title":"Recommendation systems: exploring collaborative filtering with boardgame ratings"},{"content":"","date":"Apr 21, 2023","permalink":"/professional/posts/","section":"Posts","summary":"","title":"Posts"},{"content":"\rIntro # Tensors are the foundational building blocks of machine learning and their nuances are worth spending some time on. This post is meant to be a refresher on pytorch tensors, pulling together information from various sources.\nCreating tensors # import torch import math x = torch.empty(3,4) print(type(x)) print(x) \u0026lt;class 'torch.Tensor'\u0026gt;\rtensor([[0., 0., 0., 0.],\r[0., 0., 0., 0.],\r[0., 0., 0., 0.]])\rTo start off, the pytorch docs include a great primer that begins by initializing a tensor just like the one above: \u0026gt; Let\u0026rsquo;s unpack what we just did: \u0026gt; - We created a tensor using one of the numerous factory methods attached to the torch module. \u0026gt; - The tensor itself is 2-dimensional, having 3 rows and 4 columns. \u0026gt; - The type of the object returned is torch.Tensor, which is an alias for torch.FloatTensor; by default, PyTorch tensors are populated with 32-bit floating point numbers. (More on data types below.) \u0026gt; - You will probably see some random-looking values when printing your tensor. The torch.empty() call allocates memory for the tensor, but does not initialize it with any values - so what you\u0026rsquo;re seeing is whatever was in memory at the time of allocation.\nHowever, uncontrolled random tensor starting values are often less useful than all zeros or ones. If randomness is desired, it can be fixed to be the same every time using manual_seed:\nzeros = torch.zeros(2, 3) print(zeros) ones = torch.ones(2, 3) print(ones) torch.manual_seed(1729) random = torch.rand(2, 3) print(random) tensor([[0., 0., 0.],\r[0., 0., 0.]])\rtensor([[1., 1., 1.],\r[1., 1., 1.]])\rtensor([[0.3126, 0.3791, 0.3087],\r[0.0736, 0.4216, 0.0691]])\rComparison with numpy arrays # Tensors are higher dimensional analogues to numpy arrays, sharing many qualities such as their objectives, apis, syntax, and functionality. Notable differences do exist however:\ntensors have specialized gpu processing support to accelerate operations except for broadcasting operations, tensors cannot be \u0026lsquo;jagged\u0026rsquo;, meaning having dimensions of different length all data in a tensor must be the same type Runtime and syntax errors will appear if these restrictions are ignored x + zeros RuntimeError: The size of tensor a (4) must match the size of tensor b (3) at non-singleton dimension 1 import numpy as np np.array([1, \u0026#39;two\u0026#39;, 3]) array(['1', 'two', '3'], dtype='\u0026lt;U11')\rtorch.rand([1, \u0026#39;two\u0026#39;, 3]) TypeError: rand(): argument \u0026#39;size\u0026#39; must be tuple of ints, but found element of type str at pos 2 Going beyond 3d # Higher dimensionality is difficult to visualize and often invites people to start drawing volumes and hypercubes\u0026ndash;a consequence of \u0026lsquo;dimension\u0026rsquo; in common usage as well as a sign of how far the cartesian system has permeated. How do you think beyond 3D? We can go \u0026lsquo;back to 2D\u0026rsquo;, in a sense. Forget about trying to imagine the data in shapes, connecting lines between them, and instead consider tensor dimensionaliy simply as arrays of arrays (or lists of lists).\nObserve how the output grows when we create tensors of increasing size:\ntorch.ones(2,2) tensor([[1., 1.],\r[1., 1.]])\rtorch.ones(2,2,2) tensor([[[1., 1.],\r[1., 1.]],\r[[1., 1.],\r[1., 1.]]])\rtorch.ones(2,2,2,2) tensor([[[[1., 1.],\r[1., 1.]],\r[[1., 1.],\r[1., 1.]]],\r[[[1., 1.],\r[1., 1.]],\r[[1., 1.],\r[1., 1.]]]])\rEach additional number passed to torch.ones is an additional dimension, so we\u0026rsquo;re currently at 4 dimensions. There are a few things to note from the pattern that emerges from playing around with this. These examples kept all the arguments identical but modifying the last one makes it clearer:\ntorch.ones(2,3,1,5) tensor([[[[1., 1., 1., 1., 1.]],\r[[1., 1., 1., 1., 1.]],\r[[1., 1., 1., 1., 1.]]],\r[[[1., 1., 1., 1., 1.]],\r[[1., 1., 1., 1., 1.]],\r[[1., 1., 1., 1., 1.]]]])\rthe first number determines the number of groups at the highest level the last number determines the size of the innermost elements of the tensor (here, 5) this is the same as how long each printed line is in the output for each number between the first and the last, groups are recursively nested ","date":"Apr 11, 2023","permalink":"/professional/posts/tensor-refresher/","section":"Posts","summary":"Intro # Tensors are the foundational building blocks of machine learning and their nuances are worth spending some time on.","title":"Tensor Refresher"},{"content":"","date":"Jan 1, 0001","permalink":"/professional/authors/","section":"Authors","summary":"","title":"Authors"},{"content":"","date":"Jan 1, 0001","permalink":"/professional/categories/","section":"Categories","summary":"","title":"Categories"},{"content":"","date":"Jan 1, 0001","permalink":"/professional/series/","section":"Series","summary":"","title":"Series"},{"content":"","date":"Jan 1, 0001","permalink":"/professional/tags/","section":"Tags","summary":"","title":"Tags"},{"content":"\rSummary # Data analyst and engineer with 10 years of broad technical experience: software development, cloud infrastructure, ETL, databases, automation, project management, and data visualization. Proven adaptability, effective communicator with nontechnical audiences, and believer in principled decision-making.\nExperience # Process/Data Engineer Oct \u0026#39;20 - Aug \u0026#39;22\rKite Pharma\rEnhanced manufacturing workflows by developing a custom Javascript app tailored to cell therapy needs. Collaborated with IT to connect new data sources, migrate data between Cloudera/AWS, and deploy solutions. Managed complex timelines and coordinated activities among disparate groups to successfully drive global projects.\rHighlights: drafted 2 project charters for annual budget, improved data integrity, ensured successful implementation of datalake enhancements, and resolved critical compliance risks.\rData Analyst Engineer Jul \u0026#39;19 - Jul \u0026#39;20\rRegent LP\rFacilitated acquisitions by migrating data \u0026 creating data pipelines to shared corporate infrastructure in Snowflake. Moved business units from legacy reporting systems to Looker, recreating data models and dashboards.\rHighlights: ensured data integrity for 10+ historical metrics, reduced dashboard load times by 2 minutes, and automated daily transfer of data.\rData Analyst Engineer Aug \u0026#39;17 - Jun \u0026#39;19\rThe NPD Group\rProvided data as a service for group of industry-leading clients, consistently setting the bar for data quality and supporting a critical revenue source. Wrote Python and PowerShell scripts to improve data delivery process and target data integrity issues.\rHighlights: modernized 20+ legacy ETLs to eliminate errors, automated downloading 1000+ sales files from dozens of portals on weekly basis, and migrated client on-prem databases to new platform on AWS while minimizing data discrepancies down to $0.01.\rEducation # University of California, Santa Barbara BA, English\nSkills # Languages # Python JavaScript Powershell T-SQL MySQL Pandas Seaborn Selenium Fastai Pytorch Pytest\nTechnologies # AWS Snowflake Spark Zeppelin Backblaze Impala Git Gradle Jira Intellij Idea Linux\nOther skills # SDLC TDD Agile REST\n","date":"Jan 1, 0001","permalink":"/professional/resume/","section":"luminal transubstantiation","summary":"Summary # Data analyst and engineer with 10 years of broad technical experience: software development, cloud infrastructure, ETL, databases, automation, project management, and data visualization.","title":"Tony Tran"}]